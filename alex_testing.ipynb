{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geography_based_batched\n",
    "import geography_based\n",
    "from datasets import *\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([           \n",
    "        transforms.ToTensor(),                          \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsetting first 512!\n"
     ]
    }
   ],
   "source": [
    "dataset = YfccPlacesDataset(transform_train, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(dataset=dataset, \n",
    "              num_workers=0,\n",
    "              batch_size=1,\n",
    "              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 512/512 [01:44<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "batched_res = count_langs_batch(dataloader, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old way no batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 512/512 [02:02<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "old_res = count_langs_old(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lang_counts', 'country_with_langs', 'country_with_imgs'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['United+States', 'Japan', 'Germany', 'Thailand', 'Netherlands', 'France', 'Belgium', 'Australia', 'Poland', 'United+Kingdom', 'Italy', 'New+Zealand', 'China', 'Spain', 'Switzerland', 'Indonesia', 'India', 'Nicaragua', 'Sweden', 'Peru', 'Canada', 'Brazil', 'Austria', 'Ireland', 'Croatia', 'Mexico', 'Egypt', 'Israel', 'Slovakia', 'Malaysia', 'Jamaica', 'Estonia', 'Taiwan', 'Denmark', 'Cuba', 'Norway', 'Romania', 'Sri+Lanka', 'Chile', 'Laos', 'Jordan', 'Senegal', 'Sudan', 'Serbia', 'Nepal', 'Montenegro', 'Mauritius', 'Morocco', 'Venezuela', 'Turkey', 'Russia', 'Czech+Republic', 'Finland', 'Hungary', 'United+Arab+Emirates', 'South+Korea', 'Greece', 'Angola', 'Vietnam', 'Argentina', 'Philippines', 'Lebanon', 'Portugal', 'The+Bahamas'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_res['country_with_imgs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154.2747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2818073"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (old_res['country_with_imgs']['Japan'][0])[0][0]\n",
    "test1 = (batched_res['country_with_imgs']['Japan'][0])[0][0]\n",
    "\n",
    "print(np.sum(test))\n",
    "np.sum(test1)\n",
    "\n",
    "np.mean(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sum, new_sum = 0,0\n",
    "for country in old_res['country_with_imgs']:\n",
    "    for local in range(2):\n",
    "        num_img_old = len(old_res['country_with_imgs'][country][local])\n",
    "        num_img_new = len(batched_res['country_with_imgs'][country][local])\n",
    "        num_images_match = num_img_old == num_img_new\n",
    "        if not num_images_match:\n",
    "            print(\"NUM IMAGES DONT MATCH\")\n",
    "        \n",
    "        for i in range(num_img_new):\n",
    "            # check if filepath is same\n",
    "            filepath_comp = old_res['country_with_imgs'][country][local][i][1] == batched_res['country_with_imgs'][country][local][i][1] \n",
    "            if not filepath_comp:\n",
    "                print(\"FILEPATHS DONT MATCH\")\n",
    "                \n",
    "            # check if feature array is same\n",
    "            old_feat = old_res['country_with_imgs'][country][local][i][0]\n",
    "            new_feat = batched_res['country_with_imgs'][country][local][i][0]\n",
    "            \n",
    "            allclose = np.allclose(old_feat, new_feat, atol = 0.01)\n",
    "            if not allclose:\n",
    "                print(\"Over threshold!\")\n",
    "#             print(\"element wise comparison thresh  = 0.01: {0}\".format(allclose))\n",
    "            \n",
    "#             print(np.sum(old_feat), np.sum(new_feat))\n",
    "            old_sum += np.sum(old_feat)\n",
    "            new_sum += np.sum(new_feat)\n",
    "            \n",
    "#     print(\"old: sum of all img feature vecs for {0}: {1}\".format(country, old_sum))\n",
    "#     print(\"batched: sum of all img feature vecs for {0}: {1}\".format(country, new_sum))\n",
    "    if np.abs(old_sum - new_sum) > 0.5:\n",
    "        print(\"difference in sum of feature vecs greater than 0.5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/512 [00:00<00:16, 30.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(137048.2188)\n",
      "tensor(236110.1562)\n",
      "tensor(255938.1562)\n",
      "tensor(200715.8438)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(torch.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "import re\n",
    "from countryinfo import CountryInfo\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.utils.data # changed to prevent namespace pollution\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pycountry\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def count_country(dataloader, args):\n",
    "    counts = {}\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        country = target[2][0]\n",
    "        if country not in counts.keys():\n",
    "            counts[country] = 0\n",
    "        counts[country] += 1\n",
    "\n",
    "    pickle.dump(counts, open(\"results/{}/5.pkl\".format(args.folder), \"wb\"))\n",
    "\n",
    "def count_tags(dataloader, args):\n",
    "    country_tags = {}\n",
    "    tag_to_subregion_features = {}\n",
    "    categories = dataloader.dataset.categories\n",
    "    iso3_to_subregion = pickle.load(open('util_files/iso3_to_subregion_mappings.pkl', 'rb'))\n",
    "    unique_subregions = set(list(iso3_to_subregion.values()))\n",
    "\n",
    "    # Extracts features from model pretrained on ImageNet\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    subregion_features = {}\n",
    "    for subregion in unique_subregions:\n",
    "        subregion_features[subregion] = []\n",
    "    for cat in range(len(categories)):\n",
    "        tag_to_subregion_features[cat] = copy.deepcopy(subregion_features)\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        if data is None:\n",
    "            continue\n",
    "        country = target[2][0]\n",
    "        anns = target[0]\n",
    "        filepath = target[3]\n",
    "        this_categories = list(set([categories.index(ann['label']) for ann in anns]))\n",
    "        subregion = iso3_to_subregion[country_to_iso3(country)]\n",
    "        if country not in country_tags.keys():\n",
    "            country_tags[country] = np.zeros(len(categories))\n",
    "        this_features = None\n",
    "        for cat in this_categories:\n",
    "            if len(tag_to_subregion_features[cat][subregion]) < 500:\n",
    "                data = normalize(data).to(device)\n",
    "                big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "                this_features = model.forward(big_data)\n",
    "                break\n",
    "        for cat in this_categories:\n",
    "            country_tags[country][cat] += 1\n",
    "            if this_features is not None and len(tag_to_subregion_features[cat][subregion]) < 500:\n",
    "                tag_to_subregion_features[cat][subregion].append((this_features.data.cpu().numpy(), filepath))\n",
    "\n",
    "    info_stats = {}\n",
    "    info_stats['country_tags'] = country_tags\n",
    "    info_stats['tag_to_subregion_features'] = tag_to_subregion_features\n",
    "    pickle.dump(info_stats, open(\"results/{}/6.pkl\".format(args.folder), \"wb\"))\n",
    "\n",
    "def count_langs_batch(dataloader, batch_size = 64):\n",
    "    print(\"batch method\")\n",
    "    mappings = pickle.load(open('country_lang_mappings.pkl', 'rb'))\n",
    "    iso3_to_lang = mappings['iso3_to_lang']\n",
    "    # Country to iso3 mappings that are missing\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    # IMPORANT: turn off all model stochasticity\n",
    "    model.eval()\n",
    "\n",
    "    with_country = dataloader.dataset.with_country\n",
    "\n",
    "    country_with_langs = {}\n",
    "    country_with_imgs = {} # for each country, first list is tourist second is local\n",
    "    lang_counts = {}\n",
    "    \n",
    "    # arrays to cache data for batching speedup\n",
    "    batched_big_data =[]\n",
    "    batched_country_data=[]\n",
    "    batched_local_lang_data=[]\n",
    "    batched_target_data=[]\n",
    "    \n",
    "    \n",
    "    num_data = len(dataloader)\n",
    "\n",
    "    detecter = fasttext.load_model('lid.176.bin')\n",
    "    lang_dict = {}\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            if data is None:\n",
    "                continue\n",
    "            this_tags = [tag['label'] for tag in target[0] if len(tag['label']) >= 3]\n",
    "            if len(this_tags) > 0:\n",
    "                srcz = []\n",
    "                conf = []\n",
    "                for tag in this_tags:\n",
    "                    classify = detecter.predict(tag)\n",
    "                    srcz.append(classify[0][0][9:])\n",
    "                    conf.append(classify[1][0])\n",
    "\n",
    "                # Pick out the most common language\n",
    "                commons = Counter(srcz).most_common()\n",
    "                the_src = commons[0][0]\n",
    "                # If the most common language is English, look at the second most common language\n",
    "                # since people oftentimes use English even when it's not their native language\n",
    "                if the_src == 'en' and len(commons) > 1:\n",
    "                    the_src_maybe = commons[1][0]\n",
    "                    words = [i for i in range(len(srcz)) if srcz[i] == the_src_maybe]\n",
    "                    # If this second most common language has been classified with more than .5\n",
    "                    # probability, then choose this language for the image\n",
    "                    for word in words:\n",
    "                        if conf[word] > .5: \n",
    "                            the_src = the_src_maybe\n",
    "                if the_src in lang_counts.keys():\n",
    "                    lang_counts[the_src] += 1\n",
    "                else:\n",
    "                    lang_counts[the_src] = 1\n",
    "\n",
    "                country = target[2][0]\n",
    "                iso3 = None\n",
    "                local = None\n",
    "                try:\n",
    "                    iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "                except LookupError:\n",
    "                    iso3 = missing[country]\n",
    "                try:\n",
    "                    country_info = CountryInfo(country.replace('+', ' ')).info()\n",
    "                except KeyError:\n",
    "                    country_info = {}\n",
    "                country_name = country.split('+')\n",
    "                if 'name' in country_info.keys():\n",
    "                    country_name += country_info['name']\n",
    "                if 'nativeName' in country_info.keys():\n",
    "                    country_name += country_info['nativeName']\n",
    "\n",
    "                # When comparing images to distinguish between tourist and local, we further look into the content of the tags,\n",
    "                # allowing some images to be categorized as 'unknown' if we are not that sure if it's tourist or local\n",
    "\n",
    "                # Local: in a local language, country's name isn't a tag, and 'travel' isn't a tag\n",
    "                # Tourist: in a non-local language, or 'travel' is a tag\n",
    "                try:\n",
    "                    if the_src in iso3_to_lang[iso3] and len(set(country_name)&set(this_tags)) == 0 and 'travel' not in this_tags:\n",
    "                        local = 1\n",
    "                    elif the_src not in iso3_to_lang[iso3] or 'travel' in this_tags:\n",
    "                        local = 0\n",
    "                except KeyError:\n",
    "                     print(\"This iso3 can't be found in iso3_to_lang: {}\".format(iso3))\n",
    "\n",
    "                if country not in country_with_langs.keys():\n",
    "                    country_with_langs[country] = []\n",
    "                    country_with_imgs[country] = [[], []]\n",
    "                country_with_langs[country].append(the_src)\n",
    "\n",
    "                if local is not None:\n",
    "                    if len(country_with_imgs[country][local]) < 500:\n",
    "                        data = normalize(data).to(device)\n",
    "                        big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "\n",
    "                        batched_big_data.append(big_data)\n",
    "                        batched_country_data.append(country)\n",
    "                        batched_local_lang_data.append(local)\n",
    "                        batched_target_data.append(target[3])\n",
    "\n",
    "                        if i % batch_size == 0 or i == num_data-1:\n",
    "                            all_big_data = torch.cat(batched_big_data, 0)\n",
    "                            batched_features = model(all_big_data)\n",
    "                            for i, cur_feature in enumerate(batched_features):\n",
    "                                cur_country = batched_country_data[i]\n",
    "                                cur_local = batched_local_lang_data[i]\n",
    "                                cur_target = batched_target_data[i]\n",
    "                                country_with_imgs[cur_country][cur_local].append((cur_feature.data.cpu().numpy(), cur_target))\n",
    "\n",
    "                            batched_big_data =[]\n",
    "                            batched_country_data=[]\n",
    "                            batched_local_lang_data=[]\n",
    "                            batched_target_data=[]\n",
    "\n",
    "\n",
    "    info = {}\n",
    "    info['lang_counts'] = lang_counts\n",
    "    info['country_with_langs'] = country_with_langs\n",
    "    info['country_with_imgs'] = country_with_imgs\n",
    "    \n",
    "\n",
    "    pickle.dump(info, open(\"results/testing/batched_10.pkl\", \"wb\"))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_langs_old(dataloader):\n",
    "    print(\"old way no batch\")\n",
    "    mappings = pickle.load(open('country_lang_mappings.pkl', 'rb'))\n",
    "    iso3_to_lang = mappings['iso3_to_lang']\n",
    "    # Country to iso3 mappings that are missing\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    model.eval()\n",
    "\n",
    "    with_country = dataloader.dataset.with_country\n",
    "\n",
    "    country_with_langs = {}\n",
    "    country_with_imgs = {} # for each country, first list is tourist second is local\n",
    "    lang_counts = {}\n",
    "\n",
    "    detecter = fasttext.load_model('lid.176.bin')\n",
    "    lang_dict = {}\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        if data is None:\n",
    "            continue\n",
    "        this_tags = [tag['label'] for tag in target[0] if len(tag['label']) >= 3]\n",
    "        if len(this_tags) > 0:\n",
    "            srcz = []\n",
    "            conf = []\n",
    "            for tag in this_tags:\n",
    "                classify = detecter.predict(tag)\n",
    "                srcz.append(classify[0][0][9:])\n",
    "                conf.append(classify[1][0])\n",
    "\n",
    "            # Pick out the most common language\n",
    "            commons = Counter(srcz).most_common()\n",
    "            the_src = commons[0][0]\n",
    "            # If the most common language is English, look at the second most common language\n",
    "            # since people oftentimes use English even when it's not their native language\n",
    "            if the_src == 'en' and len(commons) > 1:\n",
    "                the_src_maybe = commons[1][0]\n",
    "                words = [i for i in range(len(srcz)) if srcz[i] == the_src_maybe]\n",
    "                # If this second most common language has been classified with more than .5\n",
    "                # probability, then choose this language for the image\n",
    "                for word in words:\n",
    "                    if conf[word] > .5: \n",
    "                        the_src = the_src_maybe\n",
    "            if the_src in lang_counts.keys():\n",
    "                lang_counts[the_src] += 1\n",
    "            else:\n",
    "                lang_counts[the_src] = 1\n",
    "\n",
    "            country = target[2][0]\n",
    "            iso3 = None\n",
    "            local = None\n",
    "            try:\n",
    "                iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "            except LookupError:\n",
    "                iso3 = missing[country]\n",
    "            try:\n",
    "                country_info = CountryInfo(country.replace('+', ' ')).info()\n",
    "            except KeyError:\n",
    "                country_info = {}\n",
    "            country_name = country.split('+')\n",
    "            if 'name' in country_info.keys():\n",
    "                country_name += country_info['name']\n",
    "            if 'nativeName' in country_info.keys():\n",
    "                country_name += country_info['nativeName']\n",
    "\n",
    "            # When comparing images to distinguish between tourist and local, we further look into the content of the tags,\n",
    "            # allowing some images to be categorized as 'unknown' if we are not that sure if it's tourist or local\n",
    "\n",
    "            # Local: in a local language, country's name isn't a tag, and 'travel' isn't a tag\n",
    "            # Tourist: in a non-local language, or 'travel' is a tag\n",
    "            try:\n",
    "                if the_src in iso3_to_lang[iso3] and len(set(country_name)&set(this_tags)) == 0 and 'travel' not in this_tags:\n",
    "                    local = 1\n",
    "                elif the_src not in iso3_to_lang[iso3] or 'travel' in this_tags:\n",
    "                    local = 0\n",
    "            except KeyError:\n",
    "                 print(\"This iso3 can't be found in iso3_to_lang: {}\".format(iso3))\n",
    "\n",
    "            if country not in country_with_langs.keys():\n",
    "                country_with_langs[country] = []\n",
    "                country_with_imgs[country] = [[], []]\n",
    "            country_with_langs[country].append(the_src)\n",
    "            if local is not None:\n",
    "                if len(country_with_imgs[country][local]) < 500:\n",
    "                    data = normalize(data).to(device)\n",
    "                    big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "                    this_features = model(big_data)\n",
    "                    country_with_imgs[country][local].append((this_features.data.cpu().numpy(), target[3]))\n",
    "\n",
    "\n",
    "    info = {}\n",
    "    info['lang_counts'] = lang_counts\n",
    "    info['country_with_langs'] = country_with_langs\n",
    "    info['country_with_imgs'] = country_with_imgs\n",
    "\n",
    "    pickle.dump(info, open(\"results/testing/old_10.pkl\", \"wb\"))\n",
    "    return info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
