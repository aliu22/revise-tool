{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geography_based_batched\n",
    "import geography_based\n",
    "from datasets import *\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([           \n",
    "        transforms.ToTensor(),                          \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsetting first 1500!\n"
     ]
    }
   ],
   "source": [
    "dataset = YfccPlacesDataset(transform_train, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(dataset=dataset, \n",
    "              num_workers=0,\n",
    "              batch_size=1,\n",
    "              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/revise-scr/alex/miniconda2/envs/toolenv/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "100%|██████████| 1500/1500 [06:13<00:00,  4.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.6066975593567\n",
      "batch method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [06:39<00:00,  3.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.9880955219269\n",
      "batch method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 831/1500 [04:17<03:27,  3.23it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6b336a55471e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatched_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_langs_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c883b23a41aa>\u001b[0m in \u001b[0;36mcount_langs_batch\u001b[0;34m(dataloader, batch_size)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0miso3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_fuzzy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0miso3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/revise-scr/alex/miniconda2/envs/toolenv/lib/python3.7/site-packages/pycountry/__init__.py\u001b[0m in \u001b[0;36msearch_fuzzy\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# Some names include alternative versions which we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# match exactly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/revise-scr/alex/miniconda2/envs/toolenv/lib/python3.7/site-packages/pycountry/__init__.py\u001b[0m in \u001b[0;36mremove_accents\u001b[0;34m(input_str)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Borrowed from https://stackoverflow.com/a/517974/1509718\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnfkd_form\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFKD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mu\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfkd_form\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "batch_sz = []\n",
    "for i in [4,8,16,32,64,128,256,512]:\n",
    "    start = time.time()\n",
    "    batched_res = count_langs_batch(dataloader, i)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    times.append(end - start)\n",
    "    batch_sz.append(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_res = count_langs_old(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_res['country_with_imgs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (old_res['country_with_imgs']['Japan'][0])[0][0]\n",
    "test1 = (batched_res['country_with_imgs']['Japan'][0])[0][0]\n",
    "\n",
    "print(np.sum(test))\n",
    "np.sum(test1)\n",
    "\n",
    "np.mean(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sum, new_sum = 0,0\n",
    "for country in old_res['country_with_imgs']:\n",
    "    for local in range(2):\n",
    "        num_img_old = len(old_res['country_with_imgs'][country][local])\n",
    "        num_img_new = len(batched_res['country_with_imgs'][country][local])\n",
    "        num_images_match = num_img_old == num_img_new\n",
    "        if not num_images_match:\n",
    "            print(\"NUM IMAGES DONT MATCH\")\n",
    "        \n",
    "        for i in range(num_img_new):\n",
    "            # check if filepath is same\n",
    "            filepath_comp = old_res['country_with_imgs'][country][local][i][1] == batched_res['country_with_imgs'][country][local][i][1] \n",
    "            if not filepath_comp:\n",
    "                print(\"FILEPATHS DONT MATCH\")\n",
    "                \n",
    "            # check if feature array is same\n",
    "            old_feat = old_res['country_with_imgs'][country][local][i][0]\n",
    "            new_feat = batched_res['country_with_imgs'][country][local][i][0]\n",
    "            \n",
    "            allclose = np.allclose(old_feat, new_feat, atol = 0.01)\n",
    "            if not allclose:\n",
    "                print(\"Over threshold!\")\n",
    "#             print(\"element wise comparison thresh  = 0.01: {0}\".format(allclose))\n",
    "            \n",
    "#             print(np.sum(old_feat), np.sum(new_feat))\n",
    "            old_sum += np.sum(old_feat)\n",
    "            new_sum += np.sum(new_feat)\n",
    "            \n",
    "#     print(\"old: sum of all img feature vecs for {0}: {1}\".format(country, old_sum))\n",
    "#     print(\"batched: sum of all img feature vecs for {0}: {1}\".format(country, new_sum))\n",
    "    if np.abs(old_sum - new_sum) > 0.5:\n",
    "        print(\"difference in sum of feature vecs greater than 0.5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(torch.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "import re\n",
    "from countryinfo import CountryInfo\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.utils.data # changed to prevent namespace pollution\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pycountry\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def country_to_iso3(country):\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "    try:\n",
    "        iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "    except LookupError:\n",
    "        try:\n",
    "            iso3 = missing[country]\n",
    "        except KeyError:\n",
    "            iso3 = None\n",
    "    return iso3\n",
    "\n",
    "def count_country(dataloader, args):\n",
    "    counts = {}\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        country = target[2][0]\n",
    "        if country not in counts.keys():\n",
    "            counts[country] = 0\n",
    "        counts[country] += 1\n",
    "\n",
    "    pickle.dump(counts, open(\"results/{}/5.pkl\".format(args.folder), \"wb\"))\n",
    "\n",
    "def count_tags(dataloader, args):\n",
    "    country_tags = {}\n",
    "    tag_to_subregion_features = {}\n",
    "    categories = dataloader.dataset.categories\n",
    "    iso3_to_subregion = pickle.load(open('util_files/iso3_to_subregion_mappings.pkl', 'rb'))\n",
    "    unique_subregions = set(list(iso3_to_subregion.values()))\n",
    "\n",
    "    # Extracts features from model pretrained on ImageNet\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    subregion_features = {}\n",
    "    for subregion in unique_subregions:\n",
    "        subregion_features[subregion] = []\n",
    "    for cat in range(len(categories)):\n",
    "        tag_to_subregion_features[cat] = copy.deepcopy(subregion_features)\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        if data is None:\n",
    "            continue\n",
    "        country = target[2][0]\n",
    "        anns = target[0]\n",
    "        filepath = target[3]\n",
    "        this_categories = list(set([categories.index(ann['label']) for ann in anns]))\n",
    "        subregion = iso3_to_subregion[country_to_iso3(country)]\n",
    "        if country not in country_tags.keys():\n",
    "            country_tags[country] = np.zeros(len(categories))\n",
    "        this_features = None\n",
    "        for cat in this_categories:\n",
    "            if len(tag_to_subregion_features[cat][subregion]) < 500:\n",
    "                data = normalize(data).to(device)\n",
    "                big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "                this_features = model.forward(big_data)\n",
    "                break\n",
    "        for cat in this_categories:\n",
    "            country_tags[country][cat] += 1\n",
    "            if this_features is not None and len(tag_to_subregion_features[cat][subregion]) < 500:\n",
    "                tag_to_subregion_features[cat][subregion].append((this_features.data.cpu().numpy(), filepath))\n",
    "\n",
    "    info_stats = {}\n",
    "    info_stats['country_tags'] = country_tags\n",
    "    info_stats['tag_to_subregion_features'] = tag_to_subregion_features\n",
    "    pickle.dump(info_stats, open(\"results/{}/6.pkl\".format(args.folder), \"wb\"))\n",
    "\n",
    "def count_langs_batch(dataloader, batch_size = 64):\n",
    "    print(\"batch method\")\n",
    "    mappings = pickle.load(open('country_lang_mappings.pkl', 'rb'))\n",
    "    iso3_to_lang = mappings['iso3_to_lang']\n",
    "    # Country to iso3 mappings that are missing\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    # IMPORANT: turn off all model stochasticity\n",
    "    model.eval()\n",
    "\n",
    "    with_country = dataloader.dataset.with_country\n",
    "\n",
    "    country_with_langs = {}\n",
    "    country_with_imgs = {} # for each country, first list is tourist second is local\n",
    "    lang_counts = {}\n",
    "    \n",
    "    # arrays to cache data for batching speedup\n",
    "    batched_big_data =[]\n",
    "    batched_country_data=[]\n",
    "    batched_local_lang_data=[]\n",
    "    batched_target_data=[]\n",
    "    \n",
    "    \n",
    "    num_data = len(dataloader)\n",
    "\n",
    "    detecter = fasttext.load_model('lid.176.bin')\n",
    "    lang_dict = {}\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    print(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            if data is None:\n",
    "                continue\n",
    "            this_tags = [tag['label'] for tag in target[0] if len(tag['label']) >= 3]\n",
    "            if len(this_tags) > 0:\n",
    "                srcz = []\n",
    "                conf = []\n",
    "                for tag in this_tags:\n",
    "                    classify = detecter.predict(tag)\n",
    "                    srcz.append(classify[0][0][9:])\n",
    "                    conf.append(classify[1][0])\n",
    "\n",
    "                # Pick out the most common language\n",
    "                commons = Counter(srcz).most_common()\n",
    "                the_src = commons[0][0]\n",
    "                # If the most common language is English, look at the second most common language\n",
    "                # since people oftentimes use English even when it's not their native language\n",
    "                if the_src == 'en' and len(commons) > 1:\n",
    "                    the_src_maybe = commons[1][0]\n",
    "                    words = [i for i in range(len(srcz)) if srcz[i] == the_src_maybe]\n",
    "                    # If this second most common language has been classified with more than .5\n",
    "                    # probability, then choose this language for the image\n",
    "                    for word in words:\n",
    "                        if conf[word] > .5: \n",
    "                            the_src = the_src_maybe\n",
    "                if the_src in lang_counts.keys():\n",
    "                    lang_counts[the_src] += 1\n",
    "                else:\n",
    "                    lang_counts[the_src] = 1\n",
    "\n",
    "                country = target[2][0]\n",
    "                iso3 = None\n",
    "                local = None\n",
    "                try:\n",
    "                    iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "                except LookupError:\n",
    "                    iso3 = missing[country]\n",
    "                try:\n",
    "                    country_info = CountryInfo(country.replace('+', ' ')).info()\n",
    "                except KeyError:\n",
    "                    country_info = {}\n",
    "                country_name = country.split('+')\n",
    "                if 'name' in country_info.keys():\n",
    "                    country_name += country_info['name']\n",
    "                if 'nativeName' in country_info.keys():\n",
    "                    country_name += country_info['nativeName']\n",
    "\n",
    "                # When comparing images to distinguish between tourist and local, we further look into the content of the tags,\n",
    "                # allowing some images to be categorized as 'unknown' if we are not that sure if it's tourist or local\n",
    "\n",
    "                # Local: in a local language, country's name isn't a tag, and 'travel' isn't a tag\n",
    "                # Tourist: in a non-local language, or 'travel' is a tag\n",
    "                try:\n",
    "                    if the_src in iso3_to_lang[iso3] and len(set(country_name)&set(this_tags)) == 0 and 'travel' not in this_tags:\n",
    "                        local = 1\n",
    "                    elif the_src not in iso3_to_lang[iso3] or 'travel' in this_tags:\n",
    "                        local = 0\n",
    "                except KeyError:\n",
    "                     print(\"This iso3 can't be found in iso3_to_lang: {}\".format(iso3))\n",
    "\n",
    "                if country not in country_with_langs.keys():\n",
    "                    country_with_langs[country] = []\n",
    "                    country_with_imgs[country] = [[], []]\n",
    "                country_with_langs[country].append(the_src)\n",
    "\n",
    "                if local is not None:\n",
    "                    if len(country_with_imgs[country][local]) < 500:\n",
    "                        data = normalize(data).to(device)\n",
    "                        big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "\n",
    "                        batched_big_data.append(big_data)\n",
    "                        batched_country_data.append(country)\n",
    "                        batched_local_lang_data.append(local)\n",
    "                        batched_target_data.append(target[3])\n",
    "\n",
    "                        if i % batch_size == 0 or i == num_data-1:\n",
    "                            all_big_data = torch.cat(batched_big_data, 0)\n",
    "                            batched_features = model(all_big_data)\n",
    "                            for i, cur_feature in enumerate(batched_features):\n",
    "                                cur_country = batched_country_data[i]\n",
    "                                cur_local = batched_local_lang_data[i]\n",
    "                                cur_target = batched_target_data[i]\n",
    "                                country_with_imgs[cur_country][cur_local].append((cur_feature.data.cpu().numpy(), cur_target))\n",
    "\n",
    "                            batched_big_data =[]\n",
    "                            batched_country_data=[]\n",
    "                            batched_local_lang_data=[]\n",
    "                            batched_target_data=[]\n",
    "\n",
    "\n",
    "    info = {}\n",
    "    info['lang_counts'] = lang_counts\n",
    "    info['country_with_langs'] = country_with_langs\n",
    "    info['country_with_imgs'] = country_with_imgs\n",
    "    \n",
    "\n",
    "    pickle.dump(info, open(\"results/testing/batched_10.pkl\", \"wb\"))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_langs_old(dataloader):\n",
    "    print(\"old way no batch\")\n",
    "    mappings = pickle.load(open('country_lang_mappings.pkl', 'rb'))\n",
    "    iso3_to_lang = mappings['iso3_to_lang']\n",
    "    # Country to iso3 mappings that are missing\n",
    "    missing = {'South+Korea': 'KOR',\n",
    "            'North+Korea': 'PRK',\n",
    "            'Laos': 'LAO',\n",
    "            'Caribbean+Netherlands': 'BES',\n",
    "            'St.+Lucia': 'LCA',\n",
    "            'East+Timor': 'TLS',\n",
    "            'Democratic+Republic+of+Congo': 'COD',\n",
    "            'Swaziland': 'SWZ',\n",
    "            'Cape+Verde': 'CPV',\n",
    "            'C%C3%B4te+d%C2%B4Ivoire': 'CIV',\n",
    "            'Ivory+Coast': 'CIV',\n",
    "            'Channel+Islands': 'GBR'\n",
    "            }\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = models.alexnet(pretrained=True).to(device)\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    model.classifier = new_classifier\n",
    "    model.eval()\n",
    "\n",
    "    with_country = dataloader.dataset.with_country\n",
    "\n",
    "    country_with_langs = {}\n",
    "    country_with_imgs = {} # for each country, first list is tourist second is local\n",
    "    lang_counts = {}\n",
    "\n",
    "    detecter = fasttext.load_model('lid.176.bin')\n",
    "    lang_dict = {}\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        if data is None:\n",
    "            continue\n",
    "        this_tags = [tag['label'] for tag in target[0] if len(tag['label']) >= 3]\n",
    "        if len(this_tags) > 0:\n",
    "            srcz = []\n",
    "            conf = []\n",
    "            for tag in this_tags:\n",
    "                classify = detecter.predict(tag)\n",
    "                srcz.append(classify[0][0][9:])\n",
    "                conf.append(classify[1][0])\n",
    "\n",
    "            # Pick out the most common language\n",
    "            commons = Counter(srcz).most_common()\n",
    "            the_src = commons[0][0]\n",
    "            # If the most common language is English, look at the second most common language\n",
    "            # since people oftentimes use English even when it's not their native language\n",
    "            if the_src == 'en' and len(commons) > 1:\n",
    "                the_src_maybe = commons[1][0]\n",
    "                words = [i for i in range(len(srcz)) if srcz[i] == the_src_maybe]\n",
    "                # If this second most common language has been classified with more than .5\n",
    "                # probability, then choose this language for the image\n",
    "                for word in words:\n",
    "                    if conf[word] > .5: \n",
    "                        the_src = the_src_maybe\n",
    "            if the_src in lang_counts.keys():\n",
    "                lang_counts[the_src] += 1\n",
    "            else:\n",
    "                lang_counts[the_src] = 1\n",
    "\n",
    "            country = target[2][0]\n",
    "            iso3 = None\n",
    "            local = None\n",
    "            try:\n",
    "                iso3 = pycountry.countries.search_fuzzy(country.replace('+', ' '))[0].alpha_3\n",
    "            except LookupError:\n",
    "                iso3 = missing[country]\n",
    "            try:\n",
    "                country_info = CountryInfo(country.replace('+', ' ')).info()\n",
    "            except KeyError:\n",
    "                country_info = {}\n",
    "            country_name = country.split('+')\n",
    "            if 'name' in country_info.keys():\n",
    "                country_name += country_info['name']\n",
    "            if 'nativeName' in country_info.keys():\n",
    "                country_name += country_info['nativeName']\n",
    "\n",
    "            # When comparing images to distinguish between tourist and local, we further look into the content of the tags,\n",
    "            # allowing some images to be categorized as 'unknown' if we are not that sure if it's tourist or local\n",
    "\n",
    "            # Local: in a local language, country's name isn't a tag, and 'travel' isn't a tag\n",
    "            # Tourist: in a non-local language, or 'travel' is a tag\n",
    "            try:\n",
    "                if the_src in iso3_to_lang[iso3] and len(set(country_name)&set(this_tags)) == 0 and 'travel' not in this_tags:\n",
    "                    local = 1\n",
    "                elif the_src not in iso3_to_lang[iso3] or 'travel' in this_tags:\n",
    "                    local = 0\n",
    "            except KeyError:\n",
    "                 print(\"This iso3 can't be found in iso3_to_lang: {}\".format(iso3))\n",
    "\n",
    "            if country not in country_with_langs.keys():\n",
    "                country_with_langs[country] = []\n",
    "                country_with_imgs[country] = [[], []]\n",
    "            country_with_langs[country].append(the_src)\n",
    "            if local is not None:\n",
    "                if len(country_with_imgs[country][local]) < 500:\n",
    "                    data = normalize(data).to(device)\n",
    "                    big_data = F.interpolate(data.unsqueeze(0), size=224, mode='bilinear').to(device)\n",
    "                    this_features = model(big_data)\n",
    "                    country_with_imgs[country][local].append((this_features.data.cpu().numpy(), target[3]))\n",
    "\n",
    "\n",
    "    info = {}\n",
    "    info['lang_counts'] = lang_counts\n",
    "    info['country_with_langs'] = country_with_langs\n",
    "    info['country_with_imgs'] = country_with_imgs\n",
    "\n",
    "    pickle.dump(info, open(\"results/testing/old_10.pkl\", \"wb\"))\n",
    "    return info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
